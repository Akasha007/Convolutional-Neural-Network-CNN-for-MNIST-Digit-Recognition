{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bf041f",
   "metadata": {},
   "source": [
    "### This Notebook aims to build a CNN Model to predict the numbers from the images of the MNIST Dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41a23b",
   "metadata": {},
   "source": [
    "Work FLow:\n",
    "\n",
    "- Architecture of the network\n",
    "- Preprocess the data \n",
    "- Build and train the network\n",
    "- Test our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eef81",
   "metadata": {},
   "source": [
    "### Architecture \n",
    "\n",
    "Archutecture of our network: This means to outline what layers we are going to use, how we are going to connect them and what some of the Hyperparameters would be.\n",
    "\n",
    "Hyperparameters: \n",
    "- Kernel Size\n",
    "- Number of Kernels\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "Conv -> Maxpool -> Conv -> Maxpool -> Flatten -> Dense (10 probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e1eff",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "\n",
    "The data we have is Heavily preprocessed. However, we'll apply the following steps:\n",
    "\n",
    "- Scaling\n",
    "- Splitting the dataset\n",
    "- Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da882abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa00bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and preprocessing the data\n",
    "\n",
    "BUFFER_SIZE = 70000 #Because the dataset is small we'll keep the buffer size to be equal to the number of images\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099d07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can download the dataset using load() function from the tensorflow library\n",
    "# load() -> (dataset, meta_info)\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info = True, as_supervised = True) #as_supervised will give download the dataset in a tuple form (input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c47fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the mnist_dataset contains the whole dataset, we have to split it. Its easily achiveable as the mnist dataset is a dictionary with 2 sets labelled train and test\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495eda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that the images are in grayscale. Thus, the pixel value ranges from 0-255. We have to standardize it. \n",
    "# We'll create a function to achieve this and use this function in conjuction with the map method.\n",
    "\n",
    "# dataset.map(fn) - apply the function to every element in the dataset.\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# We didn't do anything with label but its included in the function as its required to use the map method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9502795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map method takes a function and applies it to all elements in the dataset. It does not modify the dataset but returns a new one. Thats why we need a new variable to store it.\n",
    "\n",
    "train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b583eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist does not have a validation dataset. It is very crucial while training. So lets split the dataset\n",
    "# 10% of training dataset is enough for validation\n",
    "\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ca399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the size of the validation set. We need to take that many samples from the training set and extract them into a different set.\n",
    "# There is one catch. The dataset is often sorted or odered in some way. \n",
    "# So, we'll first shuffle \n",
    "\n",
    "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "train_data = train_and_validation_data.skip(num_validation_samples)\n",
    "# This will return all the samples except for the first n (num_validation_samples) examples\n",
    "\n",
    "validation_data = train_and_validation_data.take(num_validation_samples)\n",
    "# This will do the opposite of skip. It takes the first n samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f642760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need to batch the dataset for optimal performance of the network.\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5aa12",
   "metadata": {},
   "source": [
    "Batch size is another hyperparameter that can potentially have a big impact on the training time and the final accuracy. As a general rule of thumb a bigger batch size helps speedup the train process. On the other hand, the lower batch size gives better test accuracy. It is desirable to set the batch size to a power of 2.\n",
    "\n",
    "The validation and tests set don't necesserily need to be batched as we don't backward propogate on them. However, the model expects these to be batched as well to get the proper dimensions. \n",
    "\n",
    "That's why we will batch them with the batch size equal to their number if samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae67a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202f998",
   "metadata": {},
   "source": [
    "### Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d405084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation = 'relu', input_shape = (28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Conv2D(50, 3, activation = 'relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3108551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d (Conv2D)                 (None, 24, 24, 50)            1300        \n",
      "                                                                           \n",
      " max_pooling2d (MaxPooling2D)    (None, 12, 12, 50)            0           \n",
      "                                                                           \n",
      " conv2d_1 (Conv2D)               (None, 10, 10, 50)            22550       \n",
      "                                                                           \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 50)              0           \n",
      "                                                                           \n",
      " flatten (Flatten)               (None, 1250)                  0           \n",
      "                                                                           \n",
      " dense (Dense)                   (None, 10)                    12510       \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0c5ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (None, 24, 24, 50) -> None is for batch size\n",
    "\n",
    "# Softmax is incorporated later into the loss function itself as its impossible to provide a numerically stable loss calculation for all models when using softmax output\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True) # from_logits will incorporate softmax\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = loss_fn, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d370062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we think about training, we need to think about overfitting.\n",
    "\n",
    "# In Tensorflow, early stopping is a callback\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                                  mode = 'auto',\n",
    "                                                  min_delta = 0,\n",
    "                                                  patience = 2,\n",
    "                                                  verbose = 0,\n",
    "                                                  restore_best_weights = True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bd4d1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 - 32s - loss: 0.2557 - accuracy: 0.9246 - val_loss: 0.0650 - val_accuracy: 0.9810 - 32s/epoch - 76ms/step\n",
      "Epoch 2/20\n",
      "422/422 - 31s - loss: 0.0656 - accuracy: 0.9805 - val_loss: 0.0503 - val_accuracy: 0.9843 - 31s/epoch - 73ms/step\n",
      "Epoch 3/20\n",
      "422/422 - 29s - loss: 0.0506 - accuracy: 0.9846 - val_loss: 0.0367 - val_accuracy: 0.9880 - 29s/epoch - 69ms/step\n",
      "Epoch 4/20\n",
      "422/422 - 29s - loss: 0.0421 - accuracy: 0.9869 - val_loss: 0.0332 - val_accuracy: 0.9902 - 29s/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "422/422 - 31s - loss: 0.0351 - accuracy: 0.9896 - val_loss: 0.0312 - val_accuracy: 0.9902 - 31s/epoch - 73ms/step\n",
      "Epoch 6/20\n",
      "422/422 - 31s - loss: 0.0295 - accuracy: 0.9909 - val_loss: 0.0218 - val_accuracy: 0.9935 - 31s/epoch - 73ms/step\n",
      "Epoch 7/20\n",
      "422/422 - 30s - loss: 0.0258 - accuracy: 0.9918 - val_loss: 0.0205 - val_accuracy: 0.9932 - 30s/epoch - 72ms/step\n",
      "Epoch 8/20\n",
      "422/422 - 35s - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.0176 - val_accuracy: 0.9952 - 35s/epoch - 83ms/step\n",
      "Epoch 9/20\n",
      "422/422 - 34s - loss: 0.0209 - accuracy: 0.9935 - val_loss: 0.0227 - val_accuracy: 0.9932 - 34s/epoch - 81ms/step\n",
      "Epoch 10/20\n",
      "422/422 - 29s - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.0128 - val_accuracy: 0.9955 - 29s/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "422/422 - 30s - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0183 - val_accuracy: 0.9938 - 30s/epoch - 72ms/step\n",
      "Epoch 12/20\n",
      "422/422 - 29s - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0103 - val_accuracy: 0.9965 - 29s/epoch - 68ms/step\n",
      "Epoch 13/20\n",
      "422/422 - 29s - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.0103 - val_accuracy: 0.9958 - 29s/epoch - 68ms/step\n",
      "Epoch 14/20\n",
      "422/422 - 31s - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0074 - val_accuracy: 0.9975 - 31s/epoch - 74ms/step\n",
      "Epoch 15/20\n",
      "422/422 - 37s - loss: 0.0098 - accuracy: 0.9968 - val_loss: 0.0057 - val_accuracy: 0.9978 - 37s/epoch - 89ms/step\n",
      "Epoch 16/20\n",
      "422/422 - 31s - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0040 - val_accuracy: 0.9993 - 31s/epoch - 74ms/step\n",
      "Epoch 17/20\n",
      "422/422 - 32s - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0089 - val_accuracy: 0.9988 - 32s/epoch - 75ms/step\n",
      "Epoch 18/20\n",
      "422/422 - 29s - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0058 - val_accuracy: 0.9983 - 29s/epoch - 69ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21bd30ea880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "          epochs = NUM_EPOCHS,\n",
    "          callbacks = [early_stopping], # There are more than one callback, so we use a list.\n",
    "          validation_data = validation_data,\n",
    "          verbose = 2 # This specifies what to printout during the training process. 2 will print info only at the end of each epoch. 1 will display progress bar for every batch.\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9d806",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "131bd364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0285 - accuracy: 0.9930\n",
      "Test Loss:  0.0285. Test accuracy:  99.30%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('Test Loss: {0: .4f}. Test accuracy: {1: .2f}%'.format(test_loss, test_accuracy * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd283f24",
   "metadata": {},
   "source": [
    "#### Plotting Images and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdbfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64d23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test_data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "# Reshape the images into 28x28 form, suitable for matplotlib (original dimensions: 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "515df79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAB7CAYAAACy7jQ7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD/0lEQVR4nO2dzSstcRyH77koTnnJygKrs5KUJEWksJDsRPEPeCn5FxQrWUmytxE2koUSJWEhZYWVLCyQl1Lyeu7mLu7nO93fuefOyPx8Ps/u6Zw551dP0/fMjBmJdDr9Q/Dx86sXIL4GhSdF4UlReFIUnhSFJyXX9WIikdCxnsek0+nE317THk+KwpOi8KQoPCkKT4rCk6LwpCg8KQpPisKTovCkKDwpCk+KwpOi8KQoPCkKT4rCk6LwpCg8KQpPisKTovCkKDwpCk+KwpOi8KQoPCkKT4rCk+K8TVpES0FBAXhbW5vz/U9PT+Cbm5uRrUV7PCkKT4rCk+LVjM/JyQGfmJgA7+vrc25vZ6Td/uLiAjzTUz/teqyPj4+D9/f3g5eXlzs//+3tDXxvbw+8t7cX/Orqyvl5f6I9nhSFJ0XhSUm45thXP+6ssLAQfGlpCbyjoyPS7xsdHQU/OTkB7+npAa+vrwevra2NdD3ZYn9j6HFnIoDCk6LwpMRqxldUVICvra2BV1dXO7e/vLwEHxoaAm9oaAAfHBwELy0t/ad1RsXBwQH49PQ0+O3tbVaft7W1Ba4ZLwIoPCkKT0qsZvzGxgZ4puvVdqZ3dXWBHx8fO7ff2dkBb2xszLREJ8/Pz+ALCwvgk5OT4Pbcur3+HhbNeBFA4UlReFJidT2+vb0d3P7+eH9/d77/9PT0cxb2m7u7O/DFxUXwqakp8PPz809dTxi0x5Oi8KTE6nDOruXj4wP89fUVPD8/P9T31dTUgI+NjYE/PDyAz83NgZ+dnYX6/s9Gh3MigMKTovCkxGrGz8zMgA8PDzvfPzIyAm5PkT4+PkazME/RjBcBFJ4UhSclVjO+qKgIfHl5GTzTZdrt7W3wzs5O8JeXl/9fnIdoxosACk+KwpMSqxlvKSkpAV9ZWQFvbW11br+/vw/e1NQUxbK8QTNeBFB4UhSelFjPeEsymQRfX18Hb25udm4/Pz8Pbq+/f7fjfM14EUDhSVF4Urya8Zbi4mJw+6iUTOf2W1pawHd3d6NZWEzQjBcBFJ4UhSfF6xlvqaurA7fX5+15gNnZWXD7uDPf0YwXARSeFIUnJVa3SYfl8PAQ/P7+HtzO+MrKSnD7SFB7W/Z3Qns8KQpPisKT4tWMz83F5dr71e0jQjM9orS7u9v5/uvr62yX6A3a40lReFIUnhSvztXbGX90dAReVVUV6vPLysrAfZ/xOlcvAig8KV4dztn/vGj/tGpgYAA8Ly8PPJVKga+uroLf3NyEXaI3aI8nReFJUXhSvDqcE9mhwzkRQOFJUXhSFJ4UhSdF4UlReFIUnhSFJ0XhSVF4Upzn6sX3RXs8KQpPisKTovCkKDwpCk/KL7SLIw7/kNoRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# The image to be displayed and tested\n",
    "i = 502\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5a9a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 152ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
